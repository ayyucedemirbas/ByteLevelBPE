{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bbG414GG27Xu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import json\n",
        "\n",
        "class ByteLevelBPETokenizer:\n",
        "    def __init__(self):\n",
        "        self.byte_encoder = self._bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "\n",
        "        self.bpe_merges = []\n",
        "        self.bpe_ranks = {}\n",
        "        self.vocab = {}\n",
        "        self.decoder = {}\n",
        "\n",
        "    def _bytes_to_unicode(self) -> Dict[int, str]:\n",
        "        bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"¬°\"), ord(\"¬¨\")+1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\")+1))\n",
        "        cs = bs[:]\n",
        "        n = 0\n",
        "\n",
        "        for b in range(2**8):\n",
        "            if b not in bs:\n",
        "                bs.append(b)\n",
        "                cs.append(2**8 + n)\n",
        "                n += 1\n",
        "\n",
        "        return dict(zip(bs, [chr(c) for c in cs]))\n",
        "\n",
        "    def _get_pairs(self, word: Tuple[str, ...]) -> Set[Tuple[str, str]]:\n",
        "        pairs = set()\n",
        "        prev_char = word[0]\n",
        "        for char in word[1:]:\n",
        "            pairs.add((prev_char, char))\n",
        "            prev_char = char\n",
        "        return pairs\n",
        "\n",
        "    def _basic_clean(self, text: str) -> str:\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def train(self, texts: List[str], vocab_size: int = 50000, min_frequency: int = 2):\n",
        "        word_freqs = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            text = self._basic_clean(text)\n",
        "            encoded_text = ''.join(self.byte_encoder[b] for b in text.encode('utf-8'))\n",
        "\n",
        "            words = re.findall(r'\\S+|\\s+', encoded_text)\n",
        "\n",
        "            for word in words:\n",
        "                word_tuple = tuple(word + '</w>')\n",
        "                word_freqs[word_tuple] += 1\n",
        "\n",
        "        print(f\"Found {len(word_freqs)} unique words\")\n",
        "\n",
        "        vocab = set()\n",
        "        for word in word_freqs.keys():\n",
        "            vocab.update(word)\n",
        "\n",
        "        merges = []\n",
        "\n",
        "        while len(vocab) < vocab_size:\n",
        "            pairs = defaultdict(int)\n",
        "            for word, freq in word_freqs.items():\n",
        "                word_pairs = self._get_pairs(word)\n",
        "                for pair in word_pairs:\n",
        "                    pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "\n",
        "            if pairs[best_pair] < min_frequency:\n",
        "                break\n",
        "\n",
        "            new_word_freqs = {}\n",
        "            for word, freq in word_freqs.items():\n",
        "                new_word = self._merge_word(word, best_pair)\n",
        "                new_word_freqs[new_word] = freq\n",
        "\n",
        "            word_freqs = new_word_freqs\n",
        "            merges.append(best_pair)\n",
        "            vocab.add(''.join(best_pair))\n",
        "\n",
        "            if len(merges) % 1000 == 0:\n",
        "                print(f\"Learned {len(merges)} merges, vocab size: {len(vocab)}\")\n",
        "\n",
        "        self.bpe_merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "\n",
        "        vocab_list = sorted(list(vocab))\n",
        "        self.vocab = {token: i for i, token in enumerate(vocab_list)}\n",
        "        self.decoder = {i: token for token, i in self.vocab.items()}\n",
        "\n",
        "        print(f\"{len(merges)} merges, final vocab size: {len(self.vocab)}\")\n",
        "\n",
        "    def _merge_word(self, word: Tuple[str, ...], pair: Tuple[str, str]) -> Tuple[str, ...]:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            try:\n",
        "                j = word.index(pair[0], i)\n",
        "                new_word.extend(word[i:j])\n",
        "                i = j\n",
        "            except ValueError:\n",
        "                new_word.extend(word[i:])\n",
        "                break\n",
        "\n",
        "            if i < len(word) - 1 and word[i + 1] == pair[1]:\n",
        "                new_word.append(pair[0] + pair[1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "\n",
        "        return tuple(new_word)\n",
        "\n",
        "    def _bpe(self, token: str) -> List[str]:\n",
        "        if len(token) <= 1:\n",
        "            return [token]\n",
        "\n",
        "        word = tuple(token)\n",
        "        pairs = self._get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return [token]\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "\n",
        "            word = self._merge_word(word, bigram)\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = self._get_pairs(word)\n",
        "\n",
        "        return list(word)\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        if not self.vocab:\n",
        "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
        "\n",
        "        text = self._basic_clean(text)\n",
        "        encoded_text = ''.join(self.byte_encoder[b] for b in text.encode('utf-8'))\n",
        "\n",
        "        words = re.findall(r'\\S+|\\s+', encoded_text)\n",
        "\n",
        "        bpe_tokens = []\n",
        "        for word in words:\n",
        "            word += '</w>'\n",
        "            word_tokens = self._bpe(word)\n",
        "            bpe_tokens.extend(word_tokens)\n",
        "\n",
        "        token_ids = []\n",
        "        for token in bpe_tokens:\n",
        "            if token in self.vocab:\n",
        "                token_ids.append(self.vocab[token])\n",
        "            else:\n",
        "                for char in token:\n",
        "                    if char in self.vocab:\n",
        "                        token_ids.append(self.vocab[char])\n",
        "                    else:\n",
        "                        # This shouldn't happen if training was done properly\n",
        "                        print(f\"Warning: Unknown character {repr(char)}\")\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        if not self.decoder:\n",
        "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
        "\n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id in self.decoder:\n",
        "                tokens.append(self.decoder[token_id])\n",
        "            else:\n",
        "                print(f\"Warning: Unknown token ID {token_id}\")\n",
        "\n",
        "        text = ''.join(tokens).replace('</w>', ' ')\n",
        "\n",
        "        try:\n",
        "            byte_sequence = bytes([self.byte_decoder[c] for c in text])\n",
        "            return byte_sequence.decode('utf-8')\n",
        "        except (KeyError, UnicodeDecodeError) as e:\n",
        "            print(f\"Warning: Decoding error {e}\")\n",
        "            return text\n",
        "\n",
        "    def save(self, filepath: str):\n",
        "        data = {\n",
        "            'bpe_merges': self.bpe_merges,\n",
        "            'vocab': self.vocab,\n",
        "            'byte_encoder': {str(k): v for k, v in self.byte_encoder.items()}\n",
        "        }\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def load(self, filepath: str):\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.bpe_merges = [tuple(pair) for pair in data['bpe_merges']]\n",
        "        self.bpe_ranks = {tuple(pair): i for i, pair in enumerate(self.bpe_merges)}\n",
        "        self.vocab = data['vocab']\n",
        "        self.decoder = {v: k for k, v in self.vocab.items()}\n",
        "        self.byte_encoder = {int(k): v for k, v in data['byte_encoder'].items()}\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_texts = [\n",
        "        \"Hello world! This is a sample text for training.\",\n",
        "        \"Machine learning is fascinating and powerful.\",\n",
        "        \"Natural language processing involves many interesting techniques.\",\n",
        "        \"Tokenization is a fundamental step in NLP pipelines.\",\n",
        "        \"Byte pair encoding helps create subword vocabularies.\",\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Python is a great programming language for AI.\",\n",
        "        \"This tokenizer handles any Unicode text properly.\",\n",
        "        \" \"\n",
        "]"
      ],
      "metadata": {
        "id": "ICfqUQFy56XX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer()"
      ],
      "metadata": {
        "id": "rq2pPvln5-6E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(training_texts, vocab_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otRA2zB16BN7",
        "outputId": "b53bcb60-14bb-4539-e91a-4f455f193bd9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 unique words\n",
            "66 merges, final vocab size: 108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"Hello! This is a test of our BPE tokenizer.\""
      ],
      "metadata": {
        "id": "VQ15EM3o6FLi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(test_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0y_s8SP6IYI",
        "outputId": "f0d93a55-2716-4d5c-df22-6a57cd397dca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unknown character 'E'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bauYh2Do6K7U",
        "outputId": "012a923d-d14c-4d55-e362-888bd0135d14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11,\n",
              " 37,\n",
              " 61,\n",
              " 73,\n",
              " 0,\n",
              " 105,\n",
              " 19,\n",
              " 57,\n",
              " 90,\n",
              " 86,\n",
              " 89,\n",
              " 105,\n",
              " 73,\n",
              " 44,\n",
              " 105,\n",
              " 73,\n",
              " 92,\n",
              " 85,\n",
              " 10,\n",
              " 16,\n",
              " 105,\n",
              " 89,\n",
              " 74,\n",
              " 40,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded) #number of tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bailJF4t6P1B",
        "outputId": "478b885d-16c7-4f16-d28d-6e148c1a829d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9MgY0MB6TFx",
        "outputId": "29072c39-e96d-4f7f-8a21-a9ac9e64101a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Decoding error ' '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wnaGcAmM6YlK",
        "outputId": "8a95e5cc-c36c-4b1c-fd31-aca83b88c499"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello!ƒ†Thisƒ†isƒ†aƒ†testƒ†ofƒ†ourƒ†BPƒ†tokenizer. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_text = \"H√©llo w√∂rld! üåç Testing √©mojis and √¢ccents.\""
      ],
      "metadata": {
        "id": "RFCxpaf96elE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_encoded = tokenizer.encode(special_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUCCX2Y-6imm",
        "outputId": "0ae67dda-ae01-481d-de4f-209585fee1c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unknown character '√É'\n",
            "Warning: Unknown character '¬©'\n",
            "Warning: Unknown character '√É'\n",
            "Warning: Unknown character '¬∂'\n",
            "Warning: Unknown character '√∞'\n",
            "Warning: Unknown character '≈Å'\n",
            "Warning: Unknown character 'ƒÆ'\n",
            "Warning: Unknown character 'ƒØ'\n",
            "Warning: Unknown character '√É'\n",
            "Warning: Unknown character '¬©'\n",
            "Warning: Unknown character '√É'\n",
            "Warning: Unknown character '¬¢'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_decoded = tokenizer.decode(special_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UALxiwNi6mOp",
        "outputId": "7b1826c7-d798-4285-8dc3-efc179daeba3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Decoding error ' '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c_lJ6LFk6oKQ",
        "outputId": "2e0b244f-57a5-413f-ca62-f1565c547a3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hlloƒ†wrld!ƒ†ƒ†Testingƒ†mojisƒ†andƒ†ccents. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}